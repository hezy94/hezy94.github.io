---
layout: post
title: '好未来训练营——第一周总结'
date: 2018-08-05
tags: RecommenderSystem
---

# 第一周报告

## 前言
　　过去的一周里，参加了好未来数据挖掘训练营，学习到了许多东西。不仅了解了推荐系统方面的知识，包括基于用户、基于内容、协同过滤、K最近邻居、矩阵分解。还大大锻炼了阅读英文文档的能力，Python的编程能力等等。本文是对过去一星期所学习的一些总结，内容是对个人学习的总结，来自自己与小伙伴的理解。

## 知识学习
- Day1
    - Python基础：
    因为以前接触过Python，所以回顾了一遍Python 的基础知识，简单了解了一些Python库，包括：Numpy、Pandas、Matplotlib、SciPy。还安装使用了IPython Notebook作为编辑器，体验交互式的Python编程。
    - 概率论基础：
    概率论和线性的东西以前已经学过了，所以选择了重点进行阅读。
    - 数据集——MovieLens：
    MovieLens根据不同的集合大小，不同的时间段的提供了不同的数据集。我选择了1998年4月更新的涵盖100000个评级(评分从1-5)，1000个用户的1700部电影的MovieLens 100K数据集。其中包括了用户id、电影id、电影名称、电影分类、 用户评价、评价时间的信息。
    本次学习主要用到了里边的u.data的数据集，里边为943个用户对1682部电影的评分组成的100000个数据。
    
    通过分析发现：
        - 电影评分数据中，用户最低评1分，最高评5分，平均评分3.52分。评分次数最多的是4分，接着是3分，5分
        - 观看电影最多的用户ID为547，同时被观看最多的电影ID为356。
        - ...
- Day2
    - **基于内容的推荐**：为用户推荐与他过去喜欢的产品相似的产品。一般过程包括：
        1. Item Representaton：为每个item抽取出一些特征，来表示此item
        2. Profile Learning：利用一个用户过去喜欢的item特征数据，来学习此用户的喜好特性
        3. Recommendation Generation：通过比较上一步得到的用户Profile与候选Item的特性，为此用户推荐一组相关性最大的Item。
        
        [详细的讲解——基于内容的推荐](http://www.cnblogs.com/breezedeus/archive/2012/04/10/2440488.html)

    - **协同过滤(CF)**：根据用户之前的喜好以及其他兴趣相近的用户的选择来给用户推荐商品。包括了预测与推荐，预测过程是预测用户地对没有购买过的物品的可能打分值，推荐是根据预测阶段的结果推荐用户最可能喜欢的一个或者Top-K个物品。

    [详细的讲解——协同过滤推荐算法的原理及实现](https://blog.csdn.net/yimingsilence/article/details/54934302)

    - **基于内容的推荐与CF中基于内容的推荐的区别**：
        基于内容的推荐和CF中的Item-based很相似，差别在于基于内容的推荐的item相似度是根据item的属性向量计算得到，而CF中是根据所有用户对item的评分计算得到。
    - **推荐算法的评价标准**：
        1. Top K recall:
        一般我们在推荐的时候，都是选取的分最高的前k个进行推荐，因此，这些Top K与测试集的对比，以召回率的公式计算出的结果可以评价，算法的优劣，其计算公式如下：
         R = TP/(TP+FP)
        其中的TP表示检测到的正确的数量，FP表示没有检测到的正确的值。
        2. 均方误差(Root mean square error，RMSE)：均方根误差是均方误差的算术平方根。
        计算公式如下:
        RMSE = sqrt(sum(observed-predicted)**2/N)
        3. 平均绝对误差(Mean absolute error，MAE)：绝对误差的平均值
        计算公式如下：
        MAE = sum|observed-predicted|/N
- Day 3
    - **K邻近算法(K-NearestNeighbor，KNN)**
    其核心思想是如果一个样本在特征空间中的K个最相邻的样本中的大多数属于一个类别，则该样本也属于这个类别，并具有这个类别上的样本的特性。
    - **一般步骤**：
        1. 算距离：给定测试对象，计算它与训练集中的每个对象的距离
        2. 找邻居：圈定距离最新的k个训练对象作为测试对象的近邻
        3. 做分类：根据这k个近邻归属的主要类别，对测试对象分类
    - **距离或相似度的衡量**：
        距离的衡量方法：欧式距离、夹角余弦等。对文本分类，使用余弦来计算相似度比欧式距离更合适。
    - **优点**：易于理解；易于实现，使用sklearn.neighbors包就能实现计算；无需估计参数；无需训练；适合对稀有事件进行分类；适合多分类问题。

        **缺点**：样本不平衡时，影响效果；计算量大；可解释性差。
- Day 4
    - 矩阵分解(Matrix Factorization，MF):
    其基本思想是，对于用户-物品这个评分矩阵，我们可以将其分解为用户-特征P矩阵，以及特征-物品Q矩阵。通过分解再相乘，我们就可以得到一个预测的用户-评分矩阵。
    - 具体的原理与使用参考：
        - [sklearn.decomposition.NMF](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html)
        - [小伙伴1的分享](https://shimo.im/docs/TlXEBVBuBQcecwr5/)
        - [小伙伴2的分享](https://shimo.im/docs/jRwJ8ewA7dosAk75/)
        
- Day 5
    整合了代码

    ``` python
def recommendSystem(filename,type = ''):
    header = ['userId', 'itemId', 'ratings', 'timestamp']
    df = pd.read_csv(filename, sep='\t', names=header)
    n_users = max(df['userId'])
    n_items = max(df['itemId'])
    train_data, test_data = cv.train_test_split(df, test_size=0.25)
    train_data_matrix = np.zeros((n_users, n_items))
    for line in train_data.itertuples():
        train_data_matrix[line[1] - 1, line[2] - 1] = line[3]
    test_data_matrix = np.zeros((n_users, n_items))
    for line in test_data.itertuples():
        test_data_matrix[line[1] - 1, line[2] - 1] = line[3]

    #RMSE
    def rmse(prediction, ground_truth):
        prediction = np.array(prediction[ground_truth > 0]).flatten()
        ground_truth = np.array(ground_truth[ground_truth > 0]).flatten()
        return sqrt(mean_squared_error(prediction, ground_truth))

    #MF
    def MF():
        model = NMF(n_components=50, init='random', alpha=0.0002,max_iter=200)
        W = model.fit_transform(train_data_matrix)
        H = model.components_
        M =np.dot(W,H)
        print('MF RMSE: ' + str(rmse(M, test_data_matrix)))

    #User-User CF
    def userCF():
        user_similarity = pairwise_distances(train_data_matrix, metric='cosine')
        def predict(ratings, similarity):
            mean_user_rating = ratings.mean(axis=1)
            ratings_diff = (ratings - mean_user_rating[:, np.newaxis])
            pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array(
                [np.abs(similarity).sum(axis=1)]).T
            return pred
        user_prediction = predict(train_data_matrix, user_similarity)
        print('User-User CF RMSE: ' + str(rmse(user_prediction, test_data_matrix)))

    #Item-Item CF
    def itemCF():
        item_similarity = pairwise_distances(train_data_matrix.T, metric='cosine')
        def predict(ratings, similarity):
            pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])
            return pred
        item_prediction = predict(train_data_matrix, item_similarity)
        print('Item-Item CF RMSE: ' + str(rmse(item_prediction, test_data_matrix)))

    #KNN
    def KNN():
        X_train = train_data[['userId', 'itemId']]
        y_train = train_data['ratings']
        X_test = test_data[['userId', 'itemId']]
        y_test = test_data['ratings']
        knn = neighbors.KNeighborsRegressor(5, weights='uniform')
        y_predict = knn.fit(X_train, y_train).predict(X_test)
        print('KNN RMSE: ' + str(rmse(y_predict, y_test)))
   ```
    
## 其他

- RMSE：
     - KNN ：1.128639535015498
     - userCF：3.1229881316711605
     - itemCF：3.4476505910443858
     - MF：2.828446252589667

- 代码时间：
    - KNN：105 ms ± 5.82 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
    - userCF：89.2 ms ± 787 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
    - itemCF：140 ms ± 1.2 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
    - MF：2.32 s ± 13.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

## 参考的资料
1. [DM_第二天学习记录贴_chang](https://shimo.im/docs/NdNU8nxNOmoE9IPs)
2. [基于内容的推荐](http://www.cnblogs.com/breezedeus/archive/2012/04/10/2440488.html)
3. [协同过滤推荐算法的原理及实现](https://blog.csdn.net/yimingsilence/article/details/54934302)
4. [Intro to Recommender Systems: Collaborative Filtering](http://blog.ethanrosenthal.com/2015/11/02/intro-to-collaborative-filtering/)
5. [scikit-learn学习之K最近邻算法(KNN)](https://blog.csdn.net/gamer_gyt/article/details/51232210)
6. [FutureCamp day3——zz](https://shimo.im/docs/AIZIcdIObPcYG0TN/)
7. [scikit-learn.org](http://scikit-learn.org/stable/modules/neighbors.html)
8. [What is MF?](http://www.quuxlabs.com/blog/2010/09/matrix-factorization-a-simple-tutorial-and-implementation-in-python/)
9. [How to use MF？](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html)


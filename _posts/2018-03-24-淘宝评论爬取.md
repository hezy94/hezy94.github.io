---
layout: post
title: '淘宝评论爬取'
date: 2018-03-24
tags: 爬虫
---

## 一、前言

想收集一些淘宝的数据，主要是针对评论中的好评与差评进行爬取，然后就对[淘宝网](www.taobao.com)进行爬虫。在爬虫的过程中，你会发现一路畅通，或许是淘宝自身的实力够硬吧。后来发现淘宝网的评论与天猫的评论是两个不同的地址，所以本文中的地址只是针对淘宝产品。

利用的库有：
- requests
- json
- pandas

## 二、找地址
先上淘宝找到要爬的地址：

随便找一个宝贝的地址为：

https://item.taobao.com/item.htm?spm=a230r.1.14.320.24a35be7JTTCHM&id=529187607393&ns=1&abbucket=18#detail

后来找到的规律地址为：
```
https://rate.taobao.com/feedRateList.htm?auctionNumId=XXXXXXX&rateType=YYYYYY&currentPageNum=1
```
注：
- 其中XXXXX为每个商品id的，YYYYY为评论的类型，其中1为好评，0为中评，-1为差评
- 此为淘宝的地址，天猫网站不适合。因为你会发现

## 三、爬东西
本程序中最主要的代码就是这段了。其中type为爬取的类型，filename为保存的文件名称。
```python
def getComments(type,filename):
    for i in range(len(commodityList)):
        url = commodityList[i]
        #从商品的url中获取商品id
        id = url[url.find('id=') + 3:url.find('id=') + 15]
        realUrl = 'https://rate.taobao.com/feedRateList.htm?auctionNumId=' + str(id) + '&rateType=' + type + '&currentPageNum=1'
        firstJSON = json.loads(requests.get(realUrl).text.strip().strip('()'))
        #获取的数据解析成json，获取总的评论数量
        maxNum = firstJSON['total']
        comments = []
        count = 0
        page = 1
        while count < maxNum:
            res = requests.get(realUrl[:-1] + str(page))
            page = page + 1
            jc = json.loads(res.text.strip().strip('()'))
            jc = jc['comments']
            for j in jc:
                if j['content'] == "此用户没有填写评价。":
                    count = count + 1
                    continue
                elif j['content'] == "评价方未及时做出评价,系统默认好评!":
                    count = count + 1
                    continue
                else:
                    print(j['content'])
                    comments.append(j['content'])
                    count = count + 1

        #保存评论到csv
        print('该商品共有评论' + str(len(comments)) + '条')
        pd.Series(comments).to_csv(filename,'', encoding='utf_8_sig',mode='a+')
        print('保存csv完毕')
```
注：因为淘宝中有默认好评的，所以需要加上判断，看看是否是没有评价，或者是默认好评的。
